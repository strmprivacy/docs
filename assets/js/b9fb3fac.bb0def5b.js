"use strict";(self.webpackChunkend_user_docs=self.webpackChunkend_user_docs||[]).push([[5261],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=a.createContext({}),l=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=l(e.components);return a.createElement(c.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=l(n),m=r,h=p["".concat(c,".").concat(m)]||p[m]||u[m]||o;return n?a.createElement(h,i(i({ref:t},d),{},{components:n})):a.createElement(h,i({ref:t},d))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},1130:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var a=n(7462),r=(n(7294),n(3905));const o={title:"Exporting Events",hide_table_of_contents:!1},i=void 0,s={unversionedId:"quickstart/streaming/receiving-data/batch-export",id:"quickstart/streaming/receiving-data/batch-export",title:"Exporting Events",description:"This quickstart covers how to export streaming data in batches to a blob storage location of your choice.",source:"@site/docs/03-quickstart/01-streaming/04-receiving-data/01-batch-export.md",sourceDirName:"03-quickstart/01-streaming/04-receiving-data",slug:"/quickstart/streaming/receiving-data/batch-export",permalink:"/docs/latest/quickstart/streaming/receiving-data/batch-export",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Exporting Events",hide_table_of_contents:!1},sidebar:"docs",previous:{title:"Receiving Data",permalink:"/docs/latest/quickstart/streaming/receiving-data/"},next:{title:"Exporting Keys",permalink:"/docs/latest/quickstart/streaming/receiving-data/exporting-keys"}},c={},l=[{value:"Creating a data connector",id:"creating-a-data-connector",level:2},{value:"Create a batch exporter",id:"create-a-batch-exporter",level:2},{value:"Using the results (AWS S3)",id:"using-the-results-aws-s3",level:2},{value:"About the filenames",id:"about-the-filenames",level:2},{value:"Important considerations for consumers",id:"important-considerations-for-consumers",level:2},{value:"Empty files",id:"empty-files",level:3},{value:"Handling credentials securely",id:"handling-credentials-securely",level:3},{value:"Tearing down",id:"tearing-down",level:2}],d={toc:l},p="wrapper";function u(e){let{components:t,...n}=e;return(0,r.kt)(p,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This quickstart covers how to export streaming data in batches to a blob storage location of your choice."),(0,r.kt)("admonition",{type:"important"},(0,r.kt)("p",{parentName:"admonition"},"Do not confuse ",(0,r.kt)("em",{parentName:"p"},"Batch Exporter")," with ",(0,r.kt)("em",{parentName:"p"},"Batch Jobs"),". A ",(0,r.kt)("em",{parentName:"p"},"Batch Job")," reads data from a finite source, whereas a ",(0,r.kt)("em",{parentName:"p"},"Batch\nExporter"),"\nreads data from an ",(0,r.kt)("u",null,"in"),"finite source")),(0,r.kt)("h1",{id:"exporting-data-to-aws-s3-google-cloud-storage-or-azure-blob-storage"},"Exporting data to AWS S3, Google Cloud Storage or Azure Blob Storage"),(0,r.kt)("p",null,"If you want to export streaming data to AWS S3 or other storage solutions like\nGoogle Cloud Storage or Azure Blob Storage, you first need to create a\n",(0,r.kt)("inlineCode",{parentName:"p"},"Data Connector")," pointing to your storage location.\nSee ",(0,r.kt)("a",{parentName:"p",href:"/docs/latest/quickstart/data-connectors/"},"Data Connectors")," for details for each of the supported cloud\nstorage platforms."),(0,r.kt)("h2",{id:"creating-a-data-connector"},"Creating a data connector"),(0,r.kt)("p",null,"First create a ",(0,r.kt)("a",{parentName:"p",href:"/docs/latest/quickstart/data-connectors/"},"data connector")," of the desired kind."),(0,r.kt)("h2",{id:"create-a-batch-exporter"},"Create a batch exporter"),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"batch exporter")," is the STRM Privacy component that reads a\nstream, and writes its events in batches to the target storage configured\nby your data connector."),(0,r.kt)("p",null,"Create an exporter on the ",(0,r.kt)("inlineCode",{parentName:"p"},"demo")," stream (make sure to create this\nfirst). The example below uses a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"s3"),". The type\nof connector is not of importance when creating a batch exporter for this quickstart. Just\nprovide the name of the existing data connector you want to use. Here, the path prefix ",(0,r.kt)("inlineCode",{parentName:"p"},"events")," is also provided,\nmeaning that the batch exporter will prepend blobs with this prefix."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash",metastring:"showLineNumbers",showLineNumbers:!0},'$ strm create batch-exporter demo --data-connector s3 --path-prefix events\n{\n  # callout-1\n  "ref": {\n    "name": "s3-demo" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  # callout-2\n  "streamRef": {\n    "name": "demo" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  # callout-3\n  "dataConnectorRef": {\n    "name": "s3" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  "interval": "60s",\n  "pathPrefix": "events"\n}\n')),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"ref"),": the reference of the batch exporter"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"streamRef"),": the reference to the stream that feeds the exporter"),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"dataConnectorRef"),": the reference to the data connector")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"A default name has been given to the batch exporter in this example (using the name of the data-connector and the name\nof the stream). Alternatively, a name can be set with the ",(0,r.kt)("inlineCode",{parentName:"p"},"--name")," flag.")),(0,r.kt)("p",null,"In this example, batches are being sent every 60 seconds. This can be configured with the\n",(0,r.kt)("inlineCode",{parentName:"p"},"--interval")," flag."),(0,r.kt)("p",null,"Also note that the ",(0,r.kt)("inlineCode",{parentName:"p"},"--path-prefix")," argument is optional. Make sure it\nmatches your bucket structure and permissions."),(0,r.kt)("h2",{id:"using-the-results-aws-s3"},"Using the results (AWS S3)"),(0,r.kt)("p",null,"Everything has been set up, and if you are already sending events to the stream,\nyou should start seeing data appearing in your bucket after the configured interval has\nelapsed. If you aren't sending data yet, you could simulate some random events\nwith ",(0,r.kt)("inlineCode",{parentName:"p"},"strm simulate random-events <stream name>"),"."),(0,r.kt)("p",null,"The examples below are for an S3 bucket, but the files/blobs will have the same\ncontents for other types of data connectors."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ aws s3 ls strmprivacy-export-demo/events/\n\n2021-03-26 10:56:31      79296 2021-03-26T09:56:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n2021-03-26 10:57:01     275726 2021-03-26T09:57:00-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n2021-03-26 10:57:31     277399 2021-03-26T09:57:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n")),(0,r.kt)("p",null,"And the contents of one of the files:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json",metastring:"showLineNumbers",showLineNumbers:!0},'$ aws s3 cp s3://strmprivacy-export-demo/encrypted-events/2021-03-26T09:56:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl - | head -1\n\n{\n  "strmMeta": {\n    "schemaId": "clickstream",\n    "nonce": 1009145850,\n    "timestamp": 1625820808909,\n    "keyLink": "04d243ba-2cc9-4def-b406-7241d4fce7d1",\n    "consentLevels": [\n      0,\n      1\n    ]\n  },\n  "producerSessionId": "ATqVzbsw2qN3XDj+3D0SABPPVjb2nIqCcdFcG1irE6w=",\n  "url": "https://www.strmprivacy.io/rules",\n  "eventType": "",\n  "referrer": "",\n  "userAgent": "",\n  "conversion": 0,\n  "customer": {\n    "id": "ATqVzbsKWvI9GH/rTwcI78Behpe5zo30EJMXGyEbP+u0FEZcBRwdP+A="\n  },\n  "abTests": []\n}\n')),(0,r.kt)("h2",{id:"about-the-filenames"},"About the filenames"),(0,r.kt)("p",null,"The last part of the filenames identifies the partitions being processed\nby the Kafka consumers that are doing the actual exports. When under a\nhigh event rate, and more than 1 Kafka consumer is necessary for your Batch Exporter, a\ndivision of the partitions over multiple filenames can be seen. In this example, the\ntopic has 5 partitions, and all of them are processed by one Kafka\nconsumer."),(0,r.kt)("p",null,"With manual offset management in the Kafka consumer, the probability\nof duplicate or missing data in your bucket is very low."),(0,r.kt)("h2",{id:"important-considerations-for-consumers"},"Important considerations for consumers"),(0,r.kt)("p",null,"A data connector is a very generic building block, which integrates with\nmost architectures, making it very usable. Still, there are some things to be aware of."),(0,r.kt)("h3",{id:"empty-files"},"Empty files"),(0,r.kt)("p",null,"When there are no events, a batch exporter does not write any files to\nthe data connector, so no empty files will be written."),(0,r.kt)("p",null,"However, when the batch exporter is created or (re)started, an\nempty JSONL file is created to validate the configuration (does the storage destination\nreferred to by the data connector exist\nand do the credentials grant the required access?). This results\nin ",(0,r.kt)("em",{parentName:"p"},"some")," empty files, so your downstream code needs to be able to\nhandle these. Analytics tools such as GCP BigQuery and AWS Athena are capable of dealing (i.e. ignoring) these files."),(0,r.kt)("h3",{id:"handling-credentials-securely"},"Handling credentials securely"),(0,r.kt)("p",null,"STRM Privacy stores the provided data connector credentials in a secure and encrypted\nmanner. Nevertheless, we suggest creating\ndedicated users/credentials for each data connector and/or purpose.\nGrant only the ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Principle_of_least_privilege"},"minimum required permissions")," on only the\nnecessary resources, as ",(0,r.kt)("a",{parentName:"p",href:"#creds"},"shown above"),"."),(0,r.kt)("p",null,"This way, you can easily revoke/change the credentials, and create a new data connector\nand batch exporter configuration without impacting other applications."),(0,r.kt)("h2",{id:"tearing-down"},"Tearing down"),(0,r.kt)("p",null,"Delete a batch exporter with the ",(0,r.kt)("inlineCode",{parentName:"p"},"delete")," command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete batch-exporter <batch exporter name>\nBatch Exporter has been deleted\n")),(0,r.kt)("p",null,"To delete a data connector, any dependent batch exporter needs to be deleted first.\nIt can then be deleted as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete data-connector <data connector name>\nData Connector has been deleted\n")),(0,r.kt)("p",null,"Alternatively, you can remove the data connector with all linked batch exporters in one go:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete data-connector <data connector name> --recursive\nData Connector and dependent entities have been deleted\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"You\u2019re not required to delete a data connector when deleting a batch exporter.\nAfter all, it's only a configuration item that does not do anything by itself.")))}u.isMDXComponent=!0}}]);
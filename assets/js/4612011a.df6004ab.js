"use strict";(self.webpackChunkend_user_docs=self.webpackChunkend_user_docs||[]).push([[3924],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var i=a.createContext({}),c=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),m=r,h=u["".concat(i,".").concat(m)]||u[m]||d[m]||o;return n?a.createElement(h,l(l({ref:t},p),{},{components:n})):a.createElement(h,l({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=u;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s.mdxType="string"==typeof e?e:r,l[1]=s;for(var c=2;c<o;c++)l[c]=n[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},6551:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=n(7462),r=(n(7294),n(3905));const o={title:"Batch Exporter",hide_table_of_contents:!1},l="Exporting data to S3 and other common storage solutions",s={unversionedId:"quickstart/batch-exporter",id:"quickstart/batch-exporter",title:"Batch Exporter",description:"If you want to export stream data to AWS S3 or other storage solutions like",source:"@site/docs/quickstart/batch-exporter.md",sourceDirName:"quickstart",slug:"/quickstart/batch-exporter",permalink:"/docs/latest/quickstart/batch-exporter",draft:!1,tags:[],version:"current",frontMatter:{title:"Batch Exporter",hide_table_of_contents:!1},sidebar:"docs",previous:{title:"Batch Jobs",permalink:"/docs/latest/quickstart/batch-jobs"},next:{title:"Exporting Keys",permalink:"/docs/latest/quickstart/exporting-keys"}},i={},c=[{value:"Preparation: creating target storage and credentials",id:"Preparation",level:2},{value:"1. Create the target storage",id:"bucket",level:3},{value:"2. Create the necessary credentials",id:"creds",level:3},{value:"Creating a data connector",id:"creating-a-data-connector",level:2},{value:"1. Preparation",id:"1-preparation",level:3},{value:"2. Create the data connector",id:"create-data-connector",level:3},{value:"Create a batch exporter",id:"create-a-batch-exporter",level:2},{value:"Using the results (AWS S3)",id:"using-the-results-aws-s3",level:2},{value:"About the filenames",id:"about-the-filenames",level:2},{value:"Important considerations for consumers",id:"important-considerations-for-consumers",level:2},{value:"Empty files",id:"empty-files",level:3},{value:"Handling credentials securily",id:"handling-credentials-securily",level:3},{value:"Tearing down",id:"tearing-down",level:2}],p=e=>function(t){return console.warn("Component "+e+" was not imported, exported, or provided by MDXProvider as global scope"),(0,r.kt)("div",t)},d=p("Tabs"),u=p("TabItem"),m={toc:c};function h(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"exporting-data-to-s3-and-other-common-storage-solutions"},"Exporting data to S3 and other common storage solutions"),(0,r.kt)("p",null,"If you want to export stream data to AWS S3 or other storage solutions like\nGoogle Cloud Storage or Azure Blob Storage, you first need to create a\n",(0,r.kt)("inlineCode",{parentName:"p"},"Data Connector")," pointing to a specific instance thereof (the ",(0,r.kt)("em",{parentName:"p"},"target storage"),")."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If you already have a target storage and credentials, you can skip the ",(0,r.kt)("a",{parentName:"p",href:"#Preparation"},"preparation"),"\nand directly ",(0,r.kt)("a",{parentName:"p",href:"#create-data-connector"},"create the data connector"),".")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If you don't have a target storage and/or credentials yet, but are able to create\nthem yourself, you can start from the ",(0,r.kt)("a",{parentName:"p",href:"#Preparation"},"preparation"),".")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If you need someone else to set this up for you,\nyou can forward this documentation to them, so they know what to do."))),(0,r.kt)("h2",{id:"Preparation"},"Preparation: creating target storage and credentials"),(0,r.kt)("p",null,"In this section, you will create the target storage (e.g. an AWS S3 bucket)\nand set up credentials with the required access."),(0,r.kt)("h3",{id:"bucket"},"1. Create the target storage"),(0,r.kt)(d,{groupId:"data-connector-type",mdxType:"Tabs"},(0,r.kt)(u,{value:"s3",label:"AWS S3",mdxType:"TabItem"},(0,r.kt)("p",null,"Create an S3 bucket using the command below, using your own bucket name\n(or do so in the AWS console):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ aws s3 mb s3://<your-bucket-name>\n"))),(0,r.kt)(u,{value:"s3v4",label:"S3 Compatible",mdxType:"TabItem"},(0,r.kt)("p",null,"S3 has evolved into a protocol, instead of just an Amazon product. It is possible to use our s3 data connector with\na non-AWS storage solution, as long as it provides an S3 compatible API. For example ",(0,r.kt)("a",{parentName:"p",href:"https://min.io"},"min.io"),"."),(0,r.kt)("p",null,"In this quickstart however, we will use a regular Google Cloud Storage bucket and connect using HMAC credentials. You can\nsubstitute our examples with your own storage solution."),(0,r.kt)("p",null,"Create a GCS bucket in the ",(0,r.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/storage/create-bucket"},"Google Cloud Console"),",\nor with the ",(0,r.kt)("inlineCode",{parentName:"p"},"gsutil")," CLI tool:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ gsutil mb gs://<your-bucket-name>\n"))),(0,r.kt)(u,{value:"gcs",label:"Google Cloud Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"Create a GCS bucket in the ",(0,r.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/storage/create-bucket"},"Google Cloud Console"),",\nor with the ",(0,r.kt)("inlineCode",{parentName:"p"},"gsutil")," CLI tool:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ gsutil mb gs://<your-bucket-name>\n"))),(0,r.kt)(u,{value:"azure-blob",label:"Azure Blob Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"First, create a new Blob Storage Container from the ",(0,r.kt)("a",{parentName:"p",href:"https://portal.azure.com/"},"Azure Portal"),"\nor ",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-cli"},"using the Azure CLI"),"."))),(0,r.kt)("h3",{id:"creds"},"2. Create the necessary credentials"),(0,r.kt)(d,{groupId:"data-connector-type",mdxType:"Tabs"},(0,r.kt)(u,{value:"s3",label:"AWS S3",mdxType:"TabItem"},(0,r.kt)("p",null,"Create a file with the policy document below and save it in the current\ndirectory. This file contains the permissions the data connector needs."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Action": [\n        "s3:GetBucketLocation"\n      ],\n      "Resource": "arn:aws:s3:::<your-bucket-name>"\n      //(1)\n    },\n    {\n      "Effect": "Allow",\n      "Action": [\n        "s3:PutObject"\n      ],\n      "Resource": "arn:aws:s3:::<your-bucket-name>/<optional-prefix>/*.jsonl"\n      //(2)\n    }\n  ]\n}\n')),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"fill in your bucket name"),(0,r.kt)("li",{parentName:"ol"},"fill in your bucket name ",(0,r.kt)("strong",{parentName:"li"},"and")," an optional path prefix")),(0,r.kt)("admonition",{type:"important"},(0,r.kt)("p",{parentName:"admonition"},"Make sure you replace both occurrences of ",(0,r.kt)("inlineCode",{parentName:"p"},"<your-bucket-name>")," with the\nactual name of your S3 bucket and replace the ",(0,r.kt)("inlineCode",{parentName:"p"},"<optional-prefix>")," with\nthe prefix in which STRM Privacy should put the files. If there is no\nprefix, also leave out the last slash (as a double slash will not work).")),(0,r.kt)("p",null,"The provided policy document shows the minimal set of permissions needed\nby the data connector. They are used as follows:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketLocation.html"},"GetBucketLocation"),":\nThis is an unfortunate necessity, because the AWS SDK requires us to\nconnect to the same region as from where the bucket has originally\nbeen created. STRM Privacy cannot know this in advance, so we need\nto query it using this operation.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html"},"PutObject"),":\nthe data connector will only be able to write ",(0,r.kt)("inlineCode",{parentName:"p"},"*.jsonl")," files to the specified location\n(bucket + prefix)."))),(0,r.kt)("p",null,"We don\u2019t need more permissions than these, and we also prefer to have as\nfew permissions as possible."),(0,r.kt)("p",null,"Now we need to create an IAM user which adheres to this policy. (This\nexample uses the username ",(0,r.kt)("inlineCode",{parentName:"p"},"strm-export-demo"),", but we recommend you use a\nmore descriptive name for your organization)."),(0,r.kt)("p",null,"First create the user:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws iam create-user --user-name strm-export-demo\n")),(0,r.kt)("p",null,"Then attach the above policy. This example assumes the policy\ndocument is in the same directory. Replace the file name\n",(0,r.kt)("inlineCode",{parentName:"p"},"strm-policy.json")," with the correct file name."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws iam put-user-policy --user-name strm-export-demo \\\n  --policy-name strm-bucket-write-access \\\n  --policy-document file://strm-policy.json\n")),(0,r.kt)("p",null,"Finally, create the access key for this user and download the\ncredentials (keep these safe, as they provide access to the bucket):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"aws iam create-access-key --user-name strm-export-demo > s3.json\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"STRM Privacy validates the user & credentials configuration by writing an empty JSONL file\n(file name: ",(0,r.kt)("inlineCode",{parentName:"p"},".strm_test_<random UUID>.jsonl"),") to the specified\nbucket/prefix upon creation of the batch exporter."))),(0,r.kt)(u,{value:"s3v4",label:"S3 Compatible",mdxType:"TabItem"},(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"First, create a new service account, for example in\nthe ",(0,r.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/iam-admin/serviceaccounts"},"Cloud Console"),"."),(0,r.kt)("li",{parentName:"ol"},"Then grant write permissions on the bucket to this service account. You can do this under the\n",(0,r.kt)("inlineCode",{parentName:"li"},"PERMISSIONS")," tab on the bucket's details page. Choose for example ",(0,r.kt)("inlineCode",{parentName:"li"},"Storage Legacy Bucket Writer"),"\nas role and the newly created service account as principal."),(0,r.kt)("li",{parentName:"ol"},"Create HMAC credentials: ",(0,r.kt)("inlineCode",{parentName:"li"},"gsutil hmac create <service-account-name>"),". Put the secrets in a JSON file, named for\nexample ",(0,r.kt)("inlineCode",{parentName:"li"},"s3.json"),":",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "url": "https://storage.googleapis.com",\n  "accessKey": "<access-key>",\n  "secretKey": "<secret-key>",\n  "api": "s3v4",\n  "path": "auto"\n}\n'))))),(0,r.kt)(u,{value:"gcs",label:"Google Cloud Storage",mdxType:"TabItem"},(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"First, create a new service account, for example in\nthe ",(0,r.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/iam-admin/serviceaccounts"},"Cloud Console"),"."),(0,r.kt)("li",{parentName:"ol"},"Then grant write permissions on the bucket to this service account. You can do this under the\n",(0,r.kt)("inlineCode",{parentName:"li"},"PERMISSIONS")," tab on the bucket's details page. Choose for example ",(0,r.kt)("inlineCode",{parentName:"li"},"Storage Legacy Bucket Writer"),"\nas role and the newly created service account as principal."),(0,r.kt)("li",{parentName:"ol"},"On the details page of the service account, under the ",(0,r.kt)("inlineCode",{parentName:"li"},"KEYS")," tab, select ",(0,r.kt)("inlineCode",{parentName:"li"},"ADD KEY"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"Create new key"),"\nand choose key type ",(0,r.kt)("inlineCode",{parentName:"li"},"JSON"),". After creation, the JSON credentials file is downloaded. These are the\ncredentials required when creating the data connector in the next section."))),(0,r.kt)(u,{value:"azure-blob",label:"Azure Blob Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"The STRM Privacy data connector for Azure Blob Storage currently supports Client Secret Credentials.\nWe recommend ",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal"},"creating a new Application with Service Principal"),",\n",(0,r.kt)("em",{parentName:"p"},"including a client secret"),"."),(0,r.kt)("p",null,"Next, assign the ",(0,r.kt)("inlineCode",{parentName:"p"},"Storage Blob Data Contributor")," role to this service principal, specifically for\nthe container created earlier. You can do that for example from the ",(0,r.kt)("inlineCode",{parentName:"p"},"Access Control (IAM)")," menu\nof the container in the Azure Portal."))),(0,r.kt)("h2",{id:"creating-a-data-connector"},"Creating a data connector"),(0,r.kt)("h3",{id:"1-preparation"},"1. Preparation"),(0,r.kt)(d,{groupId:"data-connector-type",mdxType:"Tabs"},(0,r.kt)(u,{value:"s3",label:"AWS S3",mdxType:"TabItem"},(0,r.kt)("p",null,"First, make sure you have a file named ",(0,r.kt)("inlineCode",{parentName:"p"},"s3.json")," in your current\ndirectory, with the following contents:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "AccessKey": {\n    "UserName": "<your user name>",\n    "AccessKeyId": "<your access key>",\n    "Status": "Active",\n    "SecretAccessKey": "<your secret access key>",\n    "CreateDate": "<the creation date>"\n  }\n}\n')),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This is the same JSON as returned by ",(0,r.kt)("inlineCode",{parentName:"p"},"aws iam create-access-key"),"."))),(0,r.kt)(u,{value:"s3v4",label:"S3 Compatible",mdxType:"TabItem"},(0,r.kt)("p",null,"If you created a JSON file with the HMAC credentials in te previous step, you are ready to create a data connector.\nIf not, create one now.")),(0,r.kt)(u,{value:"gcs",label:"Google Cloud Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"First, make sure you have a file named ",(0,r.kt)("inlineCode",{parentName:"p"},"gcs.json")," in your current directory,\ncontaining the service account credentials of the service account to use\nfor writing to the bucket."),(0,r.kt)("p",null,"The contents should look something like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "type": "service_account",\n  "project_id": "***",\n  "private_key_id": "***",\n  "private_key": "-----BEGIN PRIVATE KEY-----\\n***\\n-----END PRIVATE KEY-----\\n",\n  "client_email": "***@***.iam.gserviceaccount.com",\n  "client_id": "***",\n  "auth_uri": "https://accounts.google.com/o/oauth2/auth",\n  "token_uri": "https://oauth2.googleapis.com/token",\n  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",\n  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/***.iam.gserviceaccount.com"\n}\n'))),(0,r.kt)(u,{value:"azure-blob",label:"Azure Blob Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"To create a data connector for your Blob Storage Container, you will need the following details:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The ",(0,r.kt)("strong",{parentName:"li"},"full URI")," of your storage account (excluding container name), for example ",(0,r.kt)("inlineCode",{parentName:"li"},"https://foo.blob.core.windows.net"),"."),(0,r.kt)("li",{parentName:"ol"},"Your ",(0,r.kt)("strong",{parentName:"li"},"tenant ID"),"."),(0,r.kt)("li",{parentName:"ol"},"The ",(0,r.kt)("strong",{parentName:"li"},"client (application) ID")," of the Azure AD application used to access the container."),(0,r.kt)("li",{parentName:"ol"},"The ",(0,r.kt)("strong",{parentName:"li"},"client secret")," of the service principal used to authenticate with the AAD application.")))),(0,r.kt)("h3",{id:"create-data-connector"},"2. Create the data connector"),(0,r.kt)(d,{groupId:"data-connector-type",mdxType:"Tabs"},(0,r.kt)(u,{value:"s3",label:"AWS S3",mdxType:"TabItem"},(0,r.kt)("p",null,"With the AWS credentials in a file ",(0,r.kt)("inlineCode",{parentName:"p"},"s3.json"),", you can\ncreate the data connector using the command below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm create data-connector s3 my-s3 strmprivacy-export-demo --credentials-file=s3.json\n{\n  "ref": {\n    "name": "my-s3",\n    "projectId": "30fcd008-9696-...."\n  },\n  "s3Bucket": {\n    "bucketName": "strmprivacy-export-demo"\n  }\n}\n')),(0,r.kt)("p",null,"This will create a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"my-s3")," for the bucket ",(0,r.kt)("inlineCode",{parentName:"p"},"strmprivacy-export-demo"),",\nusing the provided credentials. Specify the actual name of your bucket, and any name for the\ndata connector itself.")),(0,r.kt)(u,{value:"s3v4",label:"S3 Compatible",mdxType:"TabItem"},(0,r.kt)("p",null,"With the credentials file (",(0,r.kt)("inlineCode",{parentName:"p"},"s3.json")," in our example), create the data connector using\nthe command below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm create data-connector s3 my-s3 strmprivacy-export-demo --credentials-file=s3.json\n{\n  "ref": {\n    "name": "my-s3",\n    "projectId": "30fcd008-9696-...."\n  },\n  "s3Bucket": {\n    "bucketName": "strmprivacy-export-demo"\n  }\n}\n')),(0,r.kt)("p",null,"This will create a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"my-s3")," for the bucket ",(0,r.kt)("inlineCode",{parentName:"p"},"strmprivacy-export-demo"),",\nusing the provided credentials. Specify the actual name of your bucket, and any name for the\ndata connector itself.")),(0,r.kt)(u,{value:"gcs",label:"Google Cloud Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"You can create the data connector with the following command, pointing to the\ncredentials file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm create data-connector gcs my-gcs strmprivacy-export-demo --credentials-file=gcs.json\n{\n  "ref": {\n    "name": "my-gcs",\n    "projectId": "30fcd008-9696-...."\n  },\n  "googleCloudStorageBucket": {\n    "bucketName": "strmprivacy-export-demo"\n  }\n}\n')),(0,r.kt)("p",null,"This will create a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"my-gcs")," for the bucket ",(0,r.kt)("inlineCode",{parentName:"p"},"strmprivacy-export-demo"),",\nusing the provided credentials.")),(0,r.kt)(u,{value:"azure-blob",label:"Azure Blob Storage",mdxType:"TabItem"},(0,r.kt)("p",null,"You can create the data connector with the following command, providing a name, the\ncontainer name, and the other required flags:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm create data-connector azure-blob-storage azure strmprivacy-export-demo --storage-account-uri "https://foo.blob.core.windows.net" --tenant-id "<your tenant ID>" --client-id "<the app client ID>" --client-secret "<the service principal\'s secret>"\n{\n  "ref": {\n    "name": "azure",\n    "projectId": "30fcd008-9696-...."\n  },\n  "azureBlobStorageContainer": {\n    "storageAccountUri": "https://foo.blob.core.windows.net",\n    "containerName": "strmprivacy-export-demo"\n  }\n}\n')),(0,r.kt)("p",null,"This will create a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"azure")," for the container ",(0,r.kt)("inlineCode",{parentName:"p"},"strmprivacy-export-demo"),",\nusing the provided client secret credentials."))),(0,r.kt)("p",null,"You can list all your data connectors with ",(0,r.kt)("inlineCode",{parentName:"p"},"strm list data-connectors"),"."),(0,r.kt)(d,{groupId:"data-connector-type",mdxType:"Tabs"},(0,r.kt)(u,{value:"s3",label:"AWS S3",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm list data-connectors -o json\n{\n  "dataConnectors": [\n    {\n      "ref": {\n        "name": "my-s3",\n        "projectId": "30fcd008-9696-...."\n      },\n      "s3Bucket": {\n        "bucketName": "strmprivacy-export-demo"\n      }\n    }\n  ]\n}\n'))),(0,r.kt)(u,{value:"s3v4",label:"S3 Compatible",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm list data-connectors -o json\n{\n  "dataConnectors": [\n    {\n      "ref": {\n        "name": "my-s3",\n        "projectId": "30fcd008-9696-...."\n      },\n      "s3Bucket": {\n        "bucketName": "strmprivacy-export-demo"\n      }\n    }\n  ]\n}\n'))),(0,r.kt)(u,{value:"gcs",label:"Google Cloud Storage",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm list data-connectors -o json\n{\n  "dataConnectors": [\n    {\n      "ref": {\n        "name": "my-gcs",\n        "projectId": "30fcd008-9696-...."\n      },\n      "googleCloudStorageBucket": {\n        "bucketName": "strmprivacy-export-demo"\n      }\n    }\n  ]\n}\n'))),(0,r.kt)(u,{value:"azure-blob",label:"Azure Blob Storage",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'$ strm list data-connectors -o json\n{\n  "dataConnectors": [\n    {\n      "ref": {\n        "name": "azure",\n        "projectId": "30fcd008-9696-...."\n      },\n      "azureBlobStorageContainer": {\n        "storageAccountUri": "https://foo.blob.core.windows.net",\n        "containerName": "strmprivacy-export-demo"\n      }\n    }\n  ]\n}\n')))),(0,r.kt)("h2",{id:"create-a-batch-exporter"},"Create a batch exporter"),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"batch exporter")," is the STRM Privacy component that reads your input\nstream, and writes its events in batches to the target storage configured\nby your data connector."),(0,r.kt)("p",null,"Let\u2019s create an exporter on the ",(0,r.kt)("inlineCode",{parentName:"p"},"demo")," stream (make sure to create this\nfirst). The below example uses a data connector named ",(0,r.kt)("inlineCode",{parentName:"p"},"s3"),". The type\nof connector is not of importance when creating a batch exporter. Just\nprovide the name of the existing data connector you want to use. Here\nwe also provide a path prefix ",(0,r.kt)("inlineCode",{parentName:"p"},"events"),", meaning that the batch exporter\nwill prepend file names with this prefix."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'strm create batch-exporter demo --data-connector s3 --path-prefix events\n{\n  "ref": { #(1)\n    "name": "s3-demo" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  "streamRef": { #(2)\n    "name": "demo" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  "dataConnectorRef": { #(3)\n    "name": "s3" ,\n    "projectId": "30fcd008-9696-...."\n  }, \n  "interval": "60s",\n  "pathPrefix": "events"\n}\n')),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"the reference of the batch exporter"),(0,r.kt)("li",{parentName:"ol"},"the reference to the stream that feeds the exporter"),(0,r.kt)("li",{parentName:"ol"},"the reference to the data connector")),(0,r.kt)("p",null,"Note that a default name has been given to the batch exporter. Alternatively,\na name can be set with the ",(0,r.kt)("inlineCode",{parentName:"p"},"--name")," flag."),(0,r.kt)("p",null,"We\u2019re sending batches every 60 seconds. This can be configured with the\n",(0,r.kt)("inlineCode",{parentName:"p"},"--interval")," flag."),(0,r.kt)("p",null,"Also note that the ",(0,r.kt)("inlineCode",{parentName:"p"},"--path-prefix")," argument is optional. Make sure it\nmatches your bucket structure and permissions."),(0,r.kt)("h2",{id:"using-the-results-aws-s3"},"Using the results (AWS S3)"),(0,r.kt)("p",null,"Everything has been set up and if you are already sending events to the stream,\nyou should start seeing data in your bucket after the configured interval has\nelapsed. If you aren't sending data yet, you could simulate some random events\nwith ",(0,r.kt)("inlineCode",{parentName:"p"},"strm simulate random-events <stream name>"),"."),(0,r.kt)("p",null,"The below examples are for an S3 bucket, but the files/blobs will have the same\ncontents for other types of data connectors."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ aws s3 ls strmprivacy-export-demo/events/\n")),(0,r.kt)("p",null,"Output:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"2021-03-26 10:56:31      79296 2021-03-26T09:56:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n2021-03-26 10:57:01     275726 2021-03-26T09:57:00-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n2021-03-26 10:57:31     277399 2021-03-26T09:57:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"In a future version, these filenames will show the ",(0,r.kt)("em",{parentName:"p"},"stream name"),",\ninstead of a uuid that we use internally.")),(0,r.kt)("p",null,"And having a look inside one of the files."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ aws s3 cp s3://strmprivacy-export-demo/encrypted-events/2021-03-26T09:56:30-stream-151daf78-eb70-4b6a-aeb4-578edc32bee6---0-1-2-3-4.jsonl - | head -1\n")),(0,r.kt)("p",null,"Output:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "strmMeta": {\n    "schemaId": "clickstream",\n    "nonce": 1009145850,\n    "timestamp": 1625820808909,\n    "keyLink": "04d243ba-2cc9-4def-b406-7241d4fce7d1",\n    "consentLevels": [\n      0,\n      1\n    ]\n  },\n  "producerSessionId": "ATqVzbsw2qN3XDj+3D0SABPPVjb2nIqCcdFcG1irE6w=",\n  "url": "https://www.strmprivacy.io/rules",\n  "eventType": "",\n  "referrer": "",\n  "userAgent": "",\n  "conversion": 0,\n  "customer": {\n    "id": "ATqVzbsKWvI9GH/rTwcI78Behpe5zo30EJMXGyEbP+u0FEZcBRwdP+A="\n  },\n  "abTests": []\n}\n')),(0,r.kt)("h2",{id:"about-the-filenames"},"About the filenames"),(0,r.kt)("p",null,"The last part of the filenames identifies the partitions being processed\nby the Kafka consumers that are doing the actual exports. When under a\nhigh event rate, we need more than 1 Kafka consumer, we would see a\ndivision of partitions over multiple filenames. In this example, the\ntopic has 5 partitions, and all of them are processed by one Kafka\nconsumer."),(0,r.kt)("p",null,"With manual offset management in the Kafka consumer, we\u2019re fairly\nconfident there will be ",(0,r.kt)("em",{parentName:"p"},"no duplicate nor missing data")," in your bucket."),(0,r.kt)("h2",{id:"important-considerations-for-consumers"},"Important considerations for consumers"),(0,r.kt)("p",null,"A data connector is a very generic building block, which integrates with\nmost architectures, making it very usable."),(0,r.kt)("p",null,"Still, there are some things to be aware of:"),(0,r.kt)("h3",{id:"empty-files"},"Empty files"),(0,r.kt)("p",null,"When there are no events, a batch exporter does not write any files to\nthe data connector, so no empty files will be written."),(0,r.kt)("p",null,"However, when the batch exporter is created or (re)started, we write an\nempty JSONL file to validate the configuration (does the storage destination\nreferred to by the data connector exist\nand do the credentials grant the required access?). This results\nin ",(0,r.kt)("em",{parentName:"p"},"some")," empty files, so your downstream code needs to be able to\nhandle these."),(0,r.kt)("h3",{id:"handling-credentials-securily"},"Handling credentials securily"),(0,r.kt)("p",null,"STRM Privacy stores the provided data connector credentials in a secure and encrypted\nmanner. Nevertheless, we suggest creating\ndedicated users/credentials for each data connector and/or purpose.\nGrant only the minimum required permissions on only the necessary resources,\nas ",(0,r.kt)("a",{parentName:"p",href:"#creds"},"shown above"),"."),(0,r.kt)("p",null,"This way, you can easily revoke/change the credentials, and create a new data connector\nand batch exporter configuration without impacting other applications."),(0,r.kt)("h2",{id:"tearing-down"},"Tearing down"),(0,r.kt)("p",null,"Delete a batch exporter with the ",(0,r.kt)("inlineCode",{parentName:"p"},"delete")," command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete batch-exporter <batch exporter name>\nBatch Exporter has been deleted\n")),(0,r.kt)("p",null,"To delete a data connector, any dependent batch exporter needs to be deleted first.\nIt can then be deleted as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete data-connector <data connector name>\nData Connector has been deleted\n")),(0,r.kt)("p",null,"Alternatively, you can remove the data connector with all linked batch exporters in one go:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ strm delete data-connector <data connector name> --recursive\nData Connector and dependent entities have been deleted\n")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"You\u2019re not required to delete a data connector when deleting a batch exporter.\nAfter all, it's only a configuration item that doesn't do anything by itself.")))}h.isMDXComponent=!0}}]);